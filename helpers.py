"""
Helper functions for displaying results and analyzing optimization performance
"""

import json
from datetime import datetime
from typing import Optional

from models import OptimizationResult


def display_optimization_results(result: OptimizationResult):
    """Display optimization results in a formatted way"""
    print("\n" + "=" * 60)
    print("üéØ PROMPT OPTIMIZATION RESULTS")
    print("=" * 60)

    if result.success:
        print(f"‚úÖ Status: SUCCESS")
        print(f"üìä Final Score: {result.final_score:.3f}")
        print(f"üîÑ Iterations: {result.iterations_completed}")
        print(f"üìà Total Versions: {len(result.all_versions)}")

        print("\n" + "-" * 40)
        print("üèÜ BEST PROMPT:")
        print("-" * 40)
        print(result.final_prompt)

        print("\n" + "-" * 40)
        print("üí≠ EVALUATION REASONING:")
        print("-" * 40)
        print(result.reasoning)

        if len(result.all_versions) > 1:
            print("\n" + "-" * 40)
            print("üìà VERSION HISTORY:")
            print("-" * 40)
            for version in result.all_versions:
                print(f"Version {version.version}: {version.average_score:.3f}")
    else:
        print(f"‚ùå Status: FAILED")
        print(f"üíî Error: {result.error_message}")

    print("\n" + "=" * 60)


def analyze_optimization_results(result: OptimizationResult):
    """Analyze and provide insights on optimization results"""

    if not result.success:
        print("‚ùå Cannot analyze failed optimization")
        return

    print("\nüìä OPTIMIZATION ANALYSIS")
    print("=" * 40)

    # Performance analysis
    scores = [
        v.average_score for v in result.all_versions if v.average_score is not None
    ]
    if len(scores) > 1:
        improvement = scores[-1] - scores[0]
        print(f"üìà Score Improvement: {improvement:+.3f}")
        print(f"üéØ Final Score: {scores[-1]:.3f}")
        print(f"ü•á Best Score: {max(scores):.3f}")

        if improvement > 0.1:
            print("‚úÖ Significant improvement achieved!")
        elif improvement > 0:
            print("‚úÖ Moderate improvement achieved")
        else:
            print("‚ö†Ô∏è No improvement or score decreased")

    # Iteration analysis
    print(f"\nüîÑ Iterations Used: {result.iterations_completed}")
    if result.iterations_completed == 1:
        print("üéØ Target achieved on first try!")
    elif result.final_score >= 0.8:
        print("üèÜ Excellent final score achieved")
    elif result.final_score >= 0.6:
        print("‚úÖ Good final score achieved")
    else:
        print("‚ö†Ô∏è Score below typical targets - consider more iterations")


def export_results_to_json(
    result: OptimizationResult, filename: Optional[str] = None
) -> Optional[str]:
    """Export optimization results to JSON file"""

    if filename is None:
        filename = f"results/prompt_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    try:
        # Convert to dict for JSON serialization
        result_dict = result.dict()

        # Add metadata
        result_dict["export_metadata"] = {
            "exported_at": datetime.now().isoformat(),
            "notebook_version": "1.0",
            "workflow_type": "modular_prompt_optimizer",
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(result_dict, f, indent=2, default=str)

        print(f"‚úÖ Results exported to: {filename}")
        return filename

    except Exception as e:
        print(f"‚ùå Export failed: {e}")
        return None
